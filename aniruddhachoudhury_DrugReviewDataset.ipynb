{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aniruddhachoudhury/DrugReviewDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnNAmqfCKfPLmj+Z0kllzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "44cf332329954359abe6f185858e99ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_30ca5e9da0004e30b8f0122a2af87ffa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da35a6e8e9e14e2589d12841b04f9206",
              "IPY_MODEL_b8d6db70d5ef48008a20ca95b4bb3570"
            ]
          }
        },
        "30ca5e9da0004e30b8f0122a2af87ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da35a6e8e9e14e2589d12841b04f9206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59e7d411bfa84bb5ac23ff4996ebc147",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1bff7c487ca4cb39a4c21578aeff37c"
          }
        },
        "b8d6db70d5ef48008a20ca95b4bb3570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_312cb76149814d99ac98dc3e18562ab2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.74kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da0dbd463ce740df98e438e2bd64c257"
          }
        },
        "59e7d411bfa84bb5ac23ff4996ebc147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1bff7c487ca4cb39a4c21578aeff37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "312cb76149814d99ac98dc3e18562ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da0dbd463ce740df98e438e2bd64c257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0caabc8a2a5460da8732717ea01799e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d00eef5e03624d6cb54f42da31ec97dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d18288fb261c42ff812d21f7b11b2917",
              "IPY_MODEL_9358468a13654766a28590b296defd27"
            ]
          }
        },
        "d00eef5e03624d6cb54f42da31ec97dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d18288fb261c42ff812d21f7b11b2917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0ce4d4d0be24b388b6d4fdbf2f65c7c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33caa38e781345c0aa87e5abdb1e747d"
          }
        },
        "9358468a13654766a28590b296defd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12e7cb4f436d406eb05c5aaf56d336fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 70.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fedc7f05022844a5892deac9543d8e31"
          }
        },
        "f0ce4d4d0be24b388b6d4fdbf2f65c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33caa38e781345c0aa87e5abdb1e747d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12e7cb4f436d406eb05c5aaf56d336fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fedc7f05022844a5892deac9543d8e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rykah14/DrugReviewDataset/blob/main/aniruddhachoudhury_DrugReviewDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLJShceY_jYM"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPjC5oBSmko1",
        "outputId": "46e92ed9-5785-4df3-a258-80a11901d834"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmFO1My4mv9Q",
        "outputId": "0a952035-c53e-44be-d138-d1ae30dec134"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 28.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=aaacca0c6f0f9c5a55934502140f0691a56bf08d5bd99b38ac8f0bfd020a13ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlsBB2C9mzjC",
        "outputId": "5cfadedb-f397-41e3-8f4f-d755ba7a1813"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=bbacdf715af1b5fe044e2c89ad246fdb784ccf084c2b7f49e67f89c5f86bba94\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTOO2_xlm-f6",
        "outputId": "1294247a-d1e4-4c3e-c347-afaf5aea16f3"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-06 02:52:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42989872 (41M) [application/x-httpd-php]\n",
            "Saving to: ‘drugsCom_raw.zip’\n",
            "\n",
            "drugsCom_raw.zip    100%[===================>]  41.00M  63.6MB/s    in 0.6s    \n",
            "\n",
            "2021-01-06 02:52:53 (63.6 MB/s) - ‘drugsCom_raw.zip’ saved [42989872/42989872]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWUKrVNnAP5",
        "outputId": "45bf904b-0b5e-4f24-f15a-94f35b9bab69"
      },
      "source": [
        "!unzip drugsCom_raw.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drugsCom_raw.zip\n",
            "  inflating: drugsComTest_raw.tsv    \n",
            "  inflating: drugsComTrain_raw.tsv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "HmriIuytnCpS",
        "outputId": "340cfb64-e3a5-45a9-9e0d-b30f32c714a9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_table(\"drugsComTest_raw.tsv\", delimiter='\\t', header=None, names=['drugName','condition','review',\t'rating'\t,'date','usefulCount'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 53,767\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>179301.0</th>\n",
              "      <td>Olanzapine</td>\n",
              "      <td>Bipolar Disorde</td>\n",
              "      <td>\"This medicine works really well as a mood sta...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>December 26, 2014</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45070.0</th>\n",
              "      <td>Fluoxetine</td>\n",
              "      <td>Anxiety and Stress</td>\n",
              "      <td>\"Ok, I felt better two days in to treatment. T...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>November 30, 2016</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157634.0</th>\n",
              "      <td>Accutane</td>\n",
              "      <td>Acne</td>\n",
              "      <td>\"Oh, thank you, God!  I am now a 48 year old f...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>November 28, 2016</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96086.0</th>\n",
              "      <td>Sertraline</td>\n",
              "      <td>Depression</td>\n",
              "      <td>\"I&amp;#039;m a 39 y.o. male who had a job for 8 y...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>May 15, 2012</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94774.0</th>\n",
              "      <td>Abilify</td>\n",
              "      <td>Depression</td>\n",
              "      <td>\"I&amp;#039;m 32 and have been diagnosed with depr...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>September 20, 2013</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141795.0</th>\n",
              "      <td>Escitalopram</td>\n",
              "      <td>Depression</td>\n",
              "      <td>\"A traumatic incident about six months ago pro...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>January 21, 2011</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148942.0</th>\n",
              "      <td>Mirena</td>\n",
              "      <td>Abnormal Uterine Bleeding</td>\n",
              "      <td>\"I have Mirena for over 6.5 years due to my he...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>January 21, 2016</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127406.0</th>\n",
              "      <td>Phentermine</td>\n",
              "      <td>Weight Loss</td>\n",
              "      <td>\"Started Phentermine 37.5mg about 10 days ago....</td>\n",
              "      <td>8.0</td>\n",
              "      <td>November 25, 2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14803.0</th>\n",
              "      <td>Ethinyl estradiol / norethindrone</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I&amp;rsquo;ve been taking this pill for ~ 1 year...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>October 7, 2017</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189600.0</th>\n",
              "      <td>Nortrel 1 / 35</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I&amp;#039;ve been on Necon 1/35 for almost two y...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>March 20, 2017</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   drugName  ... usefulCount\n",
              "179301.0                         Olanzapine  ...          29\n",
              "45070.0                          Fluoxetine  ...          96\n",
              "157634.0                           Accutane  ...          13\n",
              "96086.0                          Sertraline  ...          38\n",
              "94774.0                             Abilify  ...         198\n",
              "141795.0                       Escitalopram  ...          12\n",
              "148942.0                             Mirena  ...          10\n",
              "127406.0                        Phentermine  ...           4\n",
              "14803.0   Ethinyl estradiol / norethindrone  ...           5\n",
              "189600.0                     Nortrel 1 / 35  ...           6\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd-x2wkBrtJM"
      },
      "source": [
        "!sed -e 's/\"/'\\''/g' ./drugsComTest_raw.tsv > ./drugsComTest_raw_re.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeYz7zs4mB5I",
        "outputId": "43cf8bbe-7976-43eb-8c46-a10342ec2252"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_table('./drugsComTest_raw_re.tsv', header=None, sep='\\t', names=['drugName','condition','review',\t'rating'\t,'date','usefulCount'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['condition'].isin(['Depression','Pain']), ['condition', 'review']]\n",
        "#ADHDは削除して2値分類\n",
        "df=df.replace('Depression', 0)\n",
        "df=df.replace('Pain', 1)\n",
        "#のちのことを考慮してint型の数値にreplace\n",
        "#df=df.replace('ADHD', '2')\n",
        "print(df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        condition                                             review\n",
            "163740          0  '''I&#039;ve tried a few antidepressants over ...\n",
            "178004          1  '''Been on 30mg Cymbalta for 2 weeks. Started ...\n",
            "141462          0  '''I am a 22 year old female college student. ...\n",
            "201582          0  '''Zoloft did not help me at all.  I was on it...\n",
            "131683          0  '''Sadly only lasted 5 days on Effexor XR. The...\n",
            "...           ...                                                ...\n",
            "164760          1  '''No side effects and back to work in a coupl...\n",
            "27561           0  '''I have been taking maprotiline for over 35 ...\n",
            "28754           0  '''I&#039;m a 19 year old girl and I&#039;ve b...\n",
            "23352           1  '''Have been taking it for 6 years (120 millig...\n",
            "47656           1  '''I was prescribed Nucynta for severe neck/sh...\n",
            "\n",
            "[5195 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hqjLnuCQnbfL",
        "outputId": "03a8eb88-5858-4c0c-9fe5-63bed59e1630"
      },
      "source": [
        "df.loc[df.condition == 0].sample(5)[['condition','review']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>122017</th>\n",
              "      <td>0</td>\n",
              "      <td>'''Effexor is awesome for depression.'''</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231966</th>\n",
              "      <td>0</td>\n",
              "      <td>'''I like it. I didn&amp;#039;t notice side effect...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102943</th>\n",
              "      <td>0</td>\n",
              "      <td>'''At first I thought this medication was grea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92851</th>\n",
              "      <td>0</td>\n",
              "      <td>'''I am currently in rehab I was addicted to b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131761</th>\n",
              "      <td>0</td>\n",
              "      <td>'''originally on seroxat (paxil) for 17 years....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        condition                                             review\n",
              "122017          0           '''Effexor is awesome for depression.'''\n",
              "231966          0  '''I like it. I didn&#039;t notice side effect...\n",
              "102943          0  '''At first I thought this medication was grea...\n",
              "92851           0  '''I am currently in rehab I was addicted to b...\n",
              "131761          0  '''originally on seroxat (paxil) for 17 years...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4PBek_tnvi8"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "conditions = df.condition.values\n",
        "reviews = df.review.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeM-JgfvrcU6",
        "outputId": "1c7f02d9-7e77-45a0-cb88-2566150ec553"
      },
      "source": [
        "print(reviews[1:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'''Been on 30mg Cymbalta for 2 weeks. Started getting relief by the 2nd day. Am 58 year old male with spinal stenosis, degenerative disc disease, and spondy. Plan was to go to 60mg but I am getting enough relief at 30mg. I believe I will try to stay on that dose to minimize side effects. Some constipation, difficult reaching orgasm, some night sweats and minor headaches. Oh, insomnia if I took it at night.  I had samples of this medicine for 6 months but delayed starting it because of horror stories on the internet. Pain was ruining my life so this medicine seems to be the best option.'''\"\n",
            " \"'''I am a 22 year old female college student. I wanted to write this because when I was at my lowest of low when I felt absolutely hopeless... these positive reviews are what got me through the day. I experienced a lot of change.  I was also in a relationship that made me unhappy. I stopped doing the things I liked to do such as run, party, work, hang out with friends etc. In result, I never had energy. I constantly felt guilty. I cried everyday, sometimes multiple times of day. I went to group therapy. I dropped 10lbs in two weeks. I eventually got on this medicine &amp; the first 4 days felt crazy &amp; tired! TAKE AT NIGHT. Give this medicine time! Now 3 weeks in I am back to myself and am truly happy! Keep your head up.'''\"\n",
            " \"'''Zoloft did not help me at all.  I was on it for about 3 months, worked up to 100mg.  I was tired and hungry all the time.  I gained 20 lbs and slept all day. I was a complete zombie.  Definitely made my depression worse.'''\"\n",
            " \"'''Sadly only lasted 5 days on Effexor XR. The side effects from the 75mg dose was unlike anything I have experienced before. Within 10 hours of the first dose I had severe anxiety - something I had never experienced before. Within hours of second dose the extreme nausea came on. By day 3 I was pratically bed-ridden in cold sweats and feeling completely &quot;out of it&quot; . I persevered hoping the effects would begin to subside but by the 5th day I had severe nausea, couldn&#039;t get out of bed from feeling so sick, horrible tinnitus (ringing in the ears) and felt spaced out to the point that I began to wonder if I would ever feel &quot;normal&quot; again. Advised by GP to cease immediately on hearing my side effects.'''\"\n",
            " \"'''I have severe scar tissue and adhesions from mutiple surgeries and the pain was intense it is in my pelvic area. I fill like I&#039;m having a bad cycle every day and walking intensifies the pain this is a every day feeling. My doctor tried me on Ultram and it has been a God send. I take it three times a day. Without it it would be hard to walk so I give it a 10 thank you.'''\"\n",
            " \"'''I was first prescribed Effexor 13 years ago and was taking 225 mg. I was on it for a few years and stopped taking when I was pregnant with my first child. I was put on Paxil (gained so much weight) and then Zoloft. I took the Zoloft off and on for years until it just wasn&#039;t working for me anymore. I was then put on Welbutrin while still taking 25mg of Zoloft. I quit smoking (had no desire to smoke) but then became suicidal. My doctor then put me back on Effexor and I currently only take 75 mg. I feel amazing. Yeah the side effects are bad, I&#039;ve gained weight (lost weight the first time I was on it), have sexual side effects, and get quite shaky/jittery if I&#039;m late on a dose. But I&#039;m finally the mom I&#039;ve always wanted to be.'''\"\n",
            " \"'''I took Effexor for the first and last time yesterday around 7:41 am . At first I couldn&#039;t feel my arms or face and then it felt like fire works were going off in my head . I yawned all day long and each time I did I felt like my own tongue was gagging me . The worst of it was around 3:45 am I woke up shaking uncontrollably falling over couldn&#039;t see straight it felt like I was being electrocuted it was to the point I either wanted to be knocked out or die . My husband found me shaking and rocking back and forth in our living room floor . This drug for me personally was horrific .'''\"\n",
            " \"'''Was very beneficial when taken with a muscle relaxer for extreme pain in neck and arm. I was also taking Naproxen.'''\"\n",
            " \"'''I have been prescribed Vicodin 5/500s for over a year due to chronic headaches from a severe sinus problem.  \"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ynw9e--oOAk",
        "outputId": "ddc7e240-28d9-452e-8a11-bbe3f0ae3849"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVWEoGPfoPpF",
        "outputId": "6d9f1420-9d28-438e-865c-52f35f9a177e"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfKYmw-zoUK6",
        "outputId": "a7d51eee-68d2-4542-ad95-62a0541651be"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', reviews[10])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(reviews[10]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(reviews[10])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  '''Started taking it and I slept well at night and awoke early around 5 to start my day happily. But come 8 am and I am drowsy and needed to take already two hrs nap. Awake and resume\n",
            "Tokenized:  [\"'\", \"'\", \"'\", 'started', 'taking', 'it', 'and', 'i', 'slept', 'well', 'at', 'night', 'and', 'awoke', 'early', 'around', '5', 'to', 'start', 'my', 'day', 'happily', '.', 'but', 'come', '8', 'am', 'and', 'i', 'am', 'dr', '##ows', '##y', 'and', 'needed', 'to', 'take', 'already', 'two', 'hr', '##s', 'nap', '.', 'awake', 'and', 'resume']\n",
            "Token IDs:  [1005, 1005, 1005, 2318, 2635, 2009, 1998, 1045, 7771, 2092, 2012, 2305, 1998, 19179, 2220, 2105, 1019, 2000, 2707, 2026, 2154, 11361, 1012, 2021, 2272, 1022, 2572, 1998, 1045, 2572, 2852, 15568, 2100, 1998, 2734, 2000, 2202, 2525, 2048, 17850, 2015, 18996, 1012, 8300, 1998, 13746]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvTb4M62pFr_",
        "outputId": "1ae31dae-0d37-4a3d-ff1a-b726dc6ac181"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in reviews:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', reviews[10])\n",
        "print('Token IDs:', input_ids[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  '''Started taking it and I slept well at night and awoke early around 5 to start my day happily. But come 8 am and I am drowsy and needed to take already two hrs nap. Awake and resume\n",
            "Token IDs: [101, 1005, 1005, 1005, 2318, 2635, 2009, 1998, 1045, 7771, 2092, 2012, 2305, 1998, 19179, 2220, 2105, 1019, 2000, 2707, 2026, 2154, 11361, 1012, 2021, 2272, 1022, 2572, 1998, 1045, 2572, 2852, 15568, 2100, 1998, 2734, 2000, 2202, 2525, 2048, 17850, 2015, 18996, 1012, 8300, 1998, 13746, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ95z3iJodpM",
        "outputId": "7774ba40-9416-453b-cc28-14567fde526e"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdsuPu0-vvfk",
        "outputId": "9ac9bbe3-479a-4860-daed-0ce9194ef9ea"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzpunIbLv01M"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38A6GW3kv5cf"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_conditions, validation_conditions = train_test_split(input_ids, conditions, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, conditions,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLdBvHg8wJQn"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_conditions = torch.tensor(train_conditions)\n",
        "validation_conditions = torch.tensor(validation_conditions)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag0pjb1C4J-f"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_conditions)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_conditions)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "44cf332329954359abe6f185858e99ac",
            "30ca5e9da0004e30b8f0122a2af87ffa",
            "da35a6e8e9e14e2589d12841b04f9206",
            "b8d6db70d5ef48008a20ca95b4bb3570",
            "59e7d411bfa84bb5ac23ff4996ebc147",
            "a1bff7c487ca4cb39a4c21578aeff37c",
            "312cb76149814d99ac98dc3e18562ab2",
            "da0dbd463ce740df98e438e2bd64c257",
            "f0caabc8a2a5460da8732717ea01799e",
            "d00eef5e03624d6cb54f42da31ec97dd",
            "d18288fb261c42ff812d21f7b11b2917",
            "9358468a13654766a28590b296defd27",
            "f0ce4d4d0be24b388b6d4fdbf2f65c7c",
            "33caa38e781345c0aa87e5abdb1e747d",
            "12e7cb4f436d406eb05c5aaf56d336fb",
            "fedc7f05022844a5892deac9543d8e31"
          ]
        },
        "id": "7GVNU1HErqst",
        "outputId": "3d1d9725-9e26-4c5d-9edf-4716e356d88b"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44cf332329954359abe6f185858e99ac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0caabc8a2a5460da8732717ea01799e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXobR03zsHyh",
        "outputId": "41836752-c34c-4391-9bc6-a120853f7d71"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQmmZgnesJb0"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJLAsdgZsL_Y",
        "outputId": "bdd4b83c-cd5e-4ba7-ca2a-f8525e524a2b"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "scheduler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.optim.lr_scheduler.LambdaLR at 0x7f34ae9b6128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMgLdT0LsO8t"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FJbbti9sSNW"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Z5pm6NsUed",
        "outputId": "7a632d0a-aca6-4109-aa69-df6bf48cc290"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    147.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    147.    Elapsed: 0:00:29.\n",
            "  Batch   120  of    147.    Elapsed: 0:00:44.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    147.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    147.    Elapsed: 0:00:30.\n",
            "  Batch   120  of    147.    Elapsed: 0:00:45.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    147.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    147.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    147.    Elapsed: 0:00:47.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    147.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    147.    Elapsed: 0:00:32.\n",
            "  Batch   120  of    147.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "DFK47fx6tjxm",
        "outputId": "f0066c29-2ec3-4745-e305-34782c1592f1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}